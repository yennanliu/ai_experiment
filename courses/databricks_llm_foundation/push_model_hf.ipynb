{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOsVptvzG9A3"
      },
      "source": [
        "# LLM 02 – Prompt Tuning with PEFT\n",
        "\n",
        "This notebook demonstrates how to perform prompt tuning using the PEFT library, following the Databricks Academy example."
      ],
      "id": "pOsVptvzG9A3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hUO0TmAG9A5"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install peft transformers datasets accelerate"
      ],
      "id": "2hUO0TmAG9A5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUISI2ZnG9A6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from peft import PromptTuningConfig, get_peft_model, PromptEncoder, get_peft_model_state_dict, PeftModel"
      ],
      "id": "dUISI2ZnG9A6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRX4MT1G9A6"
      },
      "source": [
        "## Load model and tokenizer"
      ],
      "id": "qMRX4MT1G9A6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6yJRlT-G9A6"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # handle padding"
      ],
      "id": "E6yJRlT-G9A6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em6ur_lRG9A7"
      },
      "source": [
        "## Configure prompt tuning (PEFT)"
      ],
      "id": "em6ur_lRG9A7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3kyjVIjG9A7"
      },
      "outputs": [],
      "source": [
        "peft_config = PromptTuningConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    num_virtual_tokens=20,\n",
        "    prompt_encoder_hidden_size=512,\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "id": "D3kyjVIjG9A7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFAyOS4hG9A7"
      },
      "source": [
        "## Load a dataset"
      ],
      "id": "TFAyOS4hG9A7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0JOX6mBG9A7"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "id": "d0JOX6mBG9A7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5__8mG1G9A7"
      },
      "source": [
        "## Trainer setup"
      ],
      "id": "U5__8mG1G9A7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRPreh-sG9A7"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft-prompt-tuning\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    learning_rate=5e-4,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "id": "vRPreh-sG9A7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGduOM4dG9A8"
      },
      "source": [
        "## Train!"
      ],
      "id": "RGduOM4dG9A8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTxtfsqXG9A8"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "model.save_pretrained(\"./peft-prompt-tuned-model\")"
      ],
      "id": "QTxtfsqXG9A8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrAj_eVMG9A8"
      },
      "source": [
        "## Inference with the fine-tuned prompt"
      ],
      "id": "vrAj_eVMG9A8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6MtH5v-G9A8"
      },
      "outputs": [],
      "source": [
        "# Load base model + prompt adapter\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "peft_model = PeftModel.from_pretrained(base_model, \"./peft-prompt-tuned-model\")\n",
        "\n",
        "peft_model.eval()\n",
        "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")\n",
        "outputs = peft_model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "id": "O6MtH5v-G9A8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}