{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Ref \n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# https://sites.google.com/view/zsgititit/home/ji-qi-xue-xi/%E4%BD%BF%E7%94%A8python%E7%B6%93%E7%94%B1chatgpt-api%E9%80%B2%E8%A1%8C%E5%95%8F%E7%AD%94\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "\n",
    "# https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/4/summarizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use openai==0.28 for old API\n",
    "\n",
    "\n",
    "#!pip uninstall openai\n",
    "#!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Basic API Test\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2AMGnpmjxZkXPVDhjQu89q2sDYT\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505450,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 5,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Call the ChatGPT API\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "prompt = \"Tell me a joke.\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "#print(response['choices'][0]['message']['content'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Generate a Simple Post\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2ANFsMdlpZUeQZ3VXUZlGFFwzVm\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505451,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" baseball\\\").then(() => {\\n                            console.log(\\\"SEO baseball leagues\\\")\\n                        })\\n                    }, 12000, v13)\\n                })\\n            }, 3000, v12)\\n        })\\n    }, 4000, v7)\\n}, 2000);\\n//ask for scheduling === Schedule on March 30, 200\\nsetTimeout(function (v15) {\\n    setTimeout(() => {\\n        rl.question(\\\"When will you like to schedule your SEO post series(leagues)?\\\", (v16)\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 5,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"total_tokens\": 105\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"please generate a SEO post\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " baseball\").then(() => {\n",
      "                            console.log(\"SEO baseball leagues\")\n",
      "                        })\n",
      "                    }, 12000, v13)\n",
      "                })\n",
      "            }, 3000, v12)\n",
      "        })\n",
      "    }, 4000, v7)\n",
      "}, 2000);\n",
      "//ask for scheduling === Schedule on March 30, 200\n",
      "setTimeout(function (v15) {\n",
      "    setTimeout(() => {\n",
      "        rl.question(\"When will you like to schedule your SEO post series(leagues)?\", (v16)\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Generate a Simple Post\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2AOwcP05gaml1JeFJdMuSDX4vsa\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505452,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\nThe 2020 Taiwan President Election: Impact on the Semi-Conductor Industry and International Relations\\n\\nTaiwan is gearing up for its highly anticipated presidential election, which is set to take place on January 11, 2020. The election has been a hot topic of discussion, not just on the domestic front, but also on the international stage. With key issues such as the semi-conductor industry and international relations at stake, the outcome of the election is bound to have a significant impact on\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 46,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"total_tokens\": 146\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Please generate a SEO post with key words :\n",
    "  1) Taiwan president election\n",
    "  2) Semi-conductor\n",
    "  3) international relation\n",
    "  4) regional balance\n",
    "  \n",
    "  Must have 400 words\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 2020 Taiwan President Election: Impact on the Semi-Conductor Industry and International Relations\n",
      "\n",
      "Taiwan is gearing up for its highly anticipated presidential election, which is set to take place on January 11, 2020. The election has been a hot topic of discussion, not just on the domestic front, but also on the international stage. With key issues such as the semi-conductor industry and international relations at stake, the outcome of the election is bound to have a significant impact on\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Mix some artitcles\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please mix below 3 posts\n",
    "\n",
    "\n",
    "1) Kubernetes is an open-source container orchestration system for automating software deployment, scaling, and management. Originally designed by Google, the project is now maintained by the Cloud Native Computing Foundation. \n",
    "2) MLOps or ML Ops is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently. The word is a compound of \"machine learning\" and the continuous development practice of DevOps in the software field.\n",
    "3) Amazon SageMaker is a cloud based machine-learning platform that allows the creation, training, and deployment by developers of machine-learning models on the cloud. It can be used to deploy ML models on embedded systems and edge-devices. \n",
    "\n",
    "\n",
    "  Must offer insightful ideas\n",
    "  Must offer new perspective\n",
    "  \n",
    "  pleae offer result in 3000 words\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=300  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------> token = 100\n",
      "word count = 74\n",
      "\n",
      "Kubernetes, MLOps, and Amazon SageMaker are three technologies that have transformed the way we build and deploy software in today's digital age. Each of these tools offers unique benefits to developers and businesses alike. However, when combined together, they offer a powerful solution for efficient and reliable deployment of machine learning models.\n",
      "\n",
      "Kubernetes, often referred to as \"K8s\", was developed by Google in the early 2010s as a way to manage their large-scale containerized applications.\n",
      "\n",
      "\n",
      "---------------> token = 300\n",
      "word count = 244\n",
      "\n",
      "Kubernetes, MLOps, and Amazon SageMaker are all powerful tools that have revolutionized the way we approach software deployment, management, and machine learning. Each one offers unique benefits and features, but when combined, they can offer even more powerful insights and possibilities for businesses.\n",
      "\n",
      "At its core, Kubernetes is a container orchestration system that was originally designed by Google to help manage and automate the deployment and scaling of software applications. With the rise of containerization and microservices, Kubernetes has become an essential tool for businesses looking to adopt a more efficient and agile software development process.\n",
      "\n",
      "But as organizations increasingly turn to machine learning to gain insights, automate processes, and improve decision-making, they have also realized the need for a more specialized approach to deploying and managing these models in production. Enter MLOps, a merging of machine learning and DevOps principles that aims to make the deployment and maintenance of ML models more reliable and efficient.\n",
      "\n",
      "And finally, Amazon SageMaker has emerged as a leading cloud-based platform for creating, training, and deploying machine learning models. With its robust set of features and integrations, SageMaker has made it easier for developers to deploy their models in the cloud, and even on embedded systems and edge devices.\n",
      "\n",
      "But what happens when we combine these three powerful tools? What insights and possibilities can be unlocked by leveraging the strengths of Kubernetes, MLOps, and Amazon SageMaker together? Let's explore some of the possibilities in greater detail.\n",
      "\n",
      "One of the key challenges organizations face\n",
      "\n",
      "\n",
      "---------------> token = 1000\n",
      "word count = 826\n",
      "\n",
      "The world of technology is constantly evolving, and with it, new solutions and tools are emerging to help businesses stay ahead and remain competitive. Two of these powerful tools, Kubernetes and MLOps, have been gaining increasing popularity in recent years. Both are designed to streamline and automate certain aspects of software development, but with their own unique focuses. On the other hand, Amazon SageMaker is a relative newcomer in the scene, offering a cloud-based platform for deploying machine learning models. In this article, we will explore the capabilities and benefits of these three technologies and how they can work together to revolutionize the way businesses handle their operations.\n",
      "\n",
      "Kubernetes is an open-source container orchestration system that was originally designed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). Its main purpose is to automate the deployment, scaling, and management of software applications. With the rise of microservices and containerization, Kubernetes has become a go-to solution for businesses looking to modernize their IT infrastructure. Its popularity can be attributed to its ability to handle complex applications and easily scale as the needs of the business grow.\n",
      "\n",
      "One of the key benefits of Kubernetes is its self-healing capabilities. In traditional IT infrastructure, if a server goes down, the entire application may go offline. With Kubernetes, if a container fails, it automatically spins up a new one to replace it, ensuring that the application remains available. This also allows for rolling updates, where new code can be gradually rolled out to avoid any downtime.\n",
      "\n",
      "MLOps, on the other hand, stands for Machine Learning Operations. It is a paradigm that aims to streamline the development, deployment, and maintenance of machine learning models. MLOps borrows its principles from the popular software development approach, DevOps, where teams work together to continuously deliver and improve software through automated processes. In recent years, machine learning has become increasingly prevalent in various industries, and the need for a more efficient way to deploy and manage machine learning models has given rise to MLOps.\n",
      "\n",
      "One of the main benefits of MLOps is the ability to create a repeatable and standardized process for deploying machine learning models, from development to production. This means that organizations can efficiently and reliably deploy their models in real-world scenarios without manually recreating the steps each time. This enables businesses to scale their machine learning efforts and deliver more value to their customers.\n",
      "\n",
      "Finally, Amazon SageMaker is a cloud-based machine-learning platform that offers an integrated environment for data scientists and developers to build, train, and deploy machine learning models. It is designed to simplify the process of machine learning, making it accessible to a wider audience. With SageMaker, developers can quickly create their models, train them using large datasets, and deploy them to the cloud with just a few clicks.\n",
      "\n",
      "One of the key benefits of Amazon SageMaker is its ability to handle data preprocessing and feature engineering. These tasks are a crucial part of the machine learning process, and SageMaker automates them, freeing up developers' time to focus on building better models. SageMaker also offers built-in algorithms and pre-built models, making it easier for developers to get started with machine learning.\n",
      "\n",
      "Now that we have a better understanding of these three technologies, let's explore how they can work together to offer businesses a comprehensive solution for their operations. By combining Kubernetes, MLOps, and Amazon SageMaker, organizations can take their machine learning efforts to the next level.\n",
      "\n",
      "One of the most significant benefits of using Kubernetes with MLOps is the ability to easily scale machine learning models. As businesses collect more data and require more powerful models, they need a way to scale their machine learning efforts. Kubernetes is an ideal solution for this, as it can automatically spin up new containers to handle the increased workload. This means that organizations can handle an ever-growing dataset without any manual intervention. Additionally, Kubernetes can help organizations keep costs down by automatically scaling down when the workload decreases, ensuring that resources are not wasted.\n",
      "\n",
      "MLOps, with its focus on automation and continuous improvement, can bring a lot to the table when combined with Kubernetes. With MLOps, data scientists and machine learning engineers can automate the process of deploying and managing models on Kubernetes. This includes tasks like monitoring and logging, which are crucial for ensuring the reliability and performance of machine learning models. By automating these tasks, MLOps helps reduce the burden on developers and allows them to focus on building better models.\n",
      "\n",
      "The integration of Amazon SageMaker with Kubernetes and MLOps completes the trifecta of technologies that can transform the way businesses handle their operations. SageMaker offers a wide range of features that complement Kubernetes and MLOps, making it an essential part of the equation. With its powerful data preprocessing and feature engineering capabilities, SageMaker can help developers build more accurate and efficient models. Combined with Kubernetes' ability to automatically scale and MLOps' automation, this becomes a powerful tool for delivering real-world applications with machine learning capabilities.\n",
      "\n",
      "Moreover, the integration\n",
      "\n",
      "\n",
      "---------------> token = 3500\n",
      "word count = 1062\n",
      "\n",
      "Kubernetes and MLOps: The Dynamic Duo for Efficient ML Model Deployment\n",
      "\n",
      "In the ever-evolving world of technology, one of the most exciting developments in recent years has been the rise of machine learning (ML). With its ability to analyze and learn from data to make predictions and decisions, ML has become a powerful tool for businesses in various industries. However, with great power comes great responsibility, and the deployment and maintenance of ML models in production can be a daunting task. This is where Kubernetes and MLOps come in, as the dynamic duo for efficient ML model deployment. In this article, we will explore the possibilities of using these two technologies together and their potential to revolutionize the way we handle ML in production.\n",
      "\n",
      "Kubernetes, initially developed by Google, has become the de facto standard for container orchestration in the cloud. It simplifies the deployment, scaling, and management of applications by automating these processes. This allows developers to focus on writing code rather than worrying about the underlying infrastructure. On the other hand, MLOps, a term derived from the combination of \"machine learning\" and \"DevOps,\" aims to streamline the process of deploying and maintaining ML models in production. By combining the principles of DevOps with ML, MLOps automates the development and deployment of ML models, making it easier for organizations to scale their ML initiatives.\n",
      "\n",
      "So, why use Kubernetes and MLOps together? While Kubernetes handles the infrastructure layer, MLOps is responsible for the ML model layer. When used together, they form a powerful combination, and we explore below the insights and perspectives on how they can benefit businesses.\n",
      "\n",
      "Efficient Use of Resources\n",
      "One of the most significant benefits of using Kubernetes and MLOps together is the efficient use of resources. Kubernetes offers automatic scaling of resources, which means that the infrastructure can scale up or down based on demand. This is particularly useful when dealing with ML models, which can have varying resource requirements based on the dataset and complexity of the model. By using Kubernetes, organizations can ensure that they are only using the resources they need, saving them time and money.\n",
      "\n",
      "In addition, MLOps offers a streamlined process for model deployment and updates. By automating the deployment process, MLOps eliminates the need for manual intervention, which can be time-consuming and error-prone. This also leads to faster deployment times, ensuring that the latest models are available for use as soon as possible. By combining the power of Kubernetes and MLOps, organizations can deploy and manage ML models efficiently, without the need for extensive resources.\n",
      "\n",
      "Seamless Scalability\n",
      "Scalability is a crucial aspect of production ML, as models often need to handle large amounts of data and make predictions in real-time. This is where Kubernetes and MLOps shine, as they offer seamless scalability that can handle any workload. Kubernetes, with its automatic scaling capabilities, can spin up additional nodes to handle the increased workload, while MLOps handles the deployment of new models as needed.\n",
      "\n",
      "Furthermore, MLOps also offers the ability to A/B test and roll back models, ensuring that any updates or changes to the model do not have a negative impact on performance. This is especially crucial in ML, where the performance of a model can have a significant impact on the business. By using Kubernetes and MLOps together, organizations can handle varying workloads and ensure that their ML models are always performing at their best.\n",
      "\n",
      "Improved Flexibility and Portability\n",
      "In today's fast-paced business environment, agility is key, and organizations must be able to adapt quickly to changing circumstances. This is where Kubernetes and MLOps come into play, as they offer improved flexibility and portability for ML models. Kubernetes supports a wide range of platforms and services, making it easier to deploy models in different environments. This allows organizations to move their models to different cloud providers or deploy them on-premises as needed, without any significant changes to the infrastructure.\n",
      "\n",
      "Similarly, MLOps also offers flexibility and portability by automating the deployment process. This means that organizations can quickly move their models from development to production without any manual intervention, saving time and reducing the chances of errors. By using Kubernetes and MLOps, organizations can ensure that their ML models are both flexible and portable, making it easier to respond to changes in the business environment.\n",
      "\n",
      "Enhanced Collaboration and Communication\n",
      "Another insight on using Kubernetes and MLOps together is the improvement in collaboration and communication between teams. In traditional ML deployment, there can be several roadblocks in the collaboration process between data scientists, data engineers, and DevOps teams. This is because each team uses different tools and languages, hindering the communication and slowing down the deployment process.\n",
      "\n",
      "With Kubernetes and MLOps, teams can use the same platform and tooling, making collaboration and communication much smoother. This also leads to a more cohesive workflow, where teams can work together seamlessly, reducing the chances of errors and delays. By using Kubernetes and MLOps, organizations can enhance the collaboration and communication between teams, fostering a more efficient and productive work environment.\n",
      "\n",
      "Limitations and Considerations\n",
      "While Kubernetes and MLOps offer many benefits, it is essential to also consider the limitations and potential challenges of using them together. One of the main limitations is the initial learning curve for teams that are new to these technologies. Kubernetes, with its complex concepts and terminology, can be daunting for beginners, and it may take time for teams to become comfortable with it.\n",
      "\n",
      "Similarly, MLOps also has a steep learning curve, as it requires a deep understanding of ML, DevOps, and production environments. This may be challenging for organizations that do not have ML expertise in-house and may need to rely on external resources or train their teams. Furthermore, there may be compatibility issues between different versions of Kubernetes and MLOps, which may need to be resolved before deploying models.\n",
      "\n",
      "Conclusion\n",
      "The combination of Kubernetes and MLOps offers immense potential for the efficient deployment of ML models in production. By leveraging the power of Kubernetes for infrastructure management and MLOps for model deployment and updates, organizations can reap the benefits of improved resource utilization, scalability, flexibility, collaboration, and communication. However, it is essential to consider the limitations and challenges of using these technologies and ensure that teams are adequately trained to make the most out of this dynamic duo. As businesses continue to invest in ML and strive for more agile and efficient processes, the use of Kubernetes and MLOps together is bound to become a prevalent approach in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in [100, 300, 1000, 3500]:\n",
    "    response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=token  # You can adjust this parameter as needed\n",
    "    )\n",
    "    res = response[\"choices\"][0][\"text\"]\n",
    "    print()\n",
    "    print (\"---------------> token = \" + str(token))\n",
    "    print (\"word count = \" + str(len(res.split(\" \"))))\n",
    "    print (res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
