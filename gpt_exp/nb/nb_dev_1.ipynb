{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Ref \n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# https://sites.google.com/view/zsgititit/home/ji-qi-xue-xi/%E4%BD%BF%E7%94%A8python%E7%B6%93%E7%94%B1chatgpt-api%E9%80%B2%E8%A1%8C%E5%95%8F%E7%AD%94\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "\n",
    "# https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/4/summarizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use openai==0.28 for old API\n",
    "\n",
    "\n",
    "#!pip uninstall openai\n",
    "#!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Basic API Test\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2AMGnpmjxZkXPVDhjQu89q2sDYT\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505450,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 5,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Call the ChatGPT API\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "prompt = \"Tell me a joke.\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "#print(response['choices'][0]['message']['content'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Generate a Simple Post\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2ANFsMdlpZUeQZ3VXUZlGFFwzVm\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505451,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" baseball\\\").then(() => {\\n                            console.log(\\\"SEO baseball leagues\\\")\\n                        })\\n                    }, 12000, v13)\\n                })\\n            }, 3000, v12)\\n        })\\n    }, 4000, v7)\\n}, 2000);\\n//ask for scheduling === Schedule on March 30, 200\\nsetTimeout(function (v15) {\\n    setTimeout(() => {\\n        rl.question(\\\"When will you like to schedule your SEO post series(leagues)?\\\", (v16)\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 5,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"total_tokens\": 105\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"please generate a SEO post\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " baseball\").then(() => {\n",
      "                            console.log(\"SEO baseball leagues\")\n",
      "                        })\n",
      "                    }, 12000, v13)\n",
      "                })\n",
      "            }, 3000, v12)\n",
      "        })\n",
      "    }, 4000, v7)\n",
      "}, 2000);\n",
      "//ask for scheduling === Schedule on March 30, 200\n",
      "setTimeout(function (v15) {\n",
      "    setTimeout(() => {\n",
      "        rl.question(\"When will you like to schedule your SEO post series(leagues)?\", (v16)\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Generate a Simple Post\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2AOwcP05gaml1JeFJdMuSDX4vsa\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705505452,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\nThe 2020 Taiwan President Election: Impact on the Semi-Conductor Industry and International Relations\\n\\nTaiwan is gearing up for its highly anticipated presidential election, which is set to take place on January 11, 2020. The election has been a hot topic of discussion, not just on the domestic front, but also on the international stage. With key issues such as the semi-conductor industry and international relations at stake, the outcome of the election is bound to have a significant impact on\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 46,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"total_tokens\": 146\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Please generate a SEO post with key words :\n",
    "  1) Taiwan president election\n",
    "  2) Semi-conductor\n",
    "  3) international relation\n",
    "  4) regional balance\n",
    "  \n",
    "  Must have 400 words\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=100  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 2020 Taiwan President Election: Impact on the Semi-Conductor Industry and International Relations\n",
      "\n",
      "Taiwan is gearing up for its highly anticipated presidential election, which is set to take place on January 11, 2020. The election has been a hot topic of discussion, not just on the domestic front, but also on the international stage. With key issues such as the semi-conductor industry and international relations at stake, the outcome of the election is bound to have a significant impact on\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Mix some artitcles\n",
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please mix below 3 posts\n",
    "\n",
    "\n",
    "1) Kubernetes is an open-source container orchestration system for automating software deployment, scaling, and management. Originally designed by Google, the project is now maintained by the Cloud Native Computing Foundation. \n",
    "2) MLOps or ML Ops is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently. The word is a compound of \"machine learning\" and the continuous development practice of DevOps in the software field.\n",
    "3) Amazon SageMaker is a cloud based machine-learning platform that allows the creation, training, and deployment by developers of machine-learning models on the cloud. It can be used to deploy ML models on embedded systems and edge-devices. \n",
    "\n",
    "\n",
    "  Must offer insightful ideas\n",
    "  Must offer new perspective\n",
    "  \n",
    "  pleae offer result in 3000 words\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=300  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------> token = 100\n",
      "word count = 74\n",
      "\n",
      "Kubernetes, MLOps, and Amazon SageMaker are three technologies that have transformed the way we build and deploy software in today's digital age. Each of these tools offers unique benefits to developers and businesses alike. However, when combined together, they offer a powerful solution for efficient and reliable deployment of machine learning models.\n",
      "\n",
      "Kubernetes, often referred to as \"K8s\", was developed by Google in the early 2010s as a way to manage their large-scale containerized applications.\n",
      "\n",
      "\n",
      "---------------> token = 300\n",
      "word count = 244\n",
      "\n",
      "Kubernetes, MLOps, and Amazon SageMaker are all powerful tools that have revolutionized the way we approach software deployment, management, and machine learning. Each one offers unique benefits and features, but when combined, they can offer even more powerful insights and possibilities for businesses.\n",
      "\n",
      "At its core, Kubernetes is a container orchestration system that was originally designed by Google to help manage and automate the deployment and scaling of software applications. With the rise of containerization and microservices, Kubernetes has become an essential tool for businesses looking to adopt a more efficient and agile software development process.\n",
      "\n",
      "But as organizations increasingly turn to machine learning to gain insights, automate processes, and improve decision-making, they have also realized the need for a more specialized approach to deploying and managing these models in production. Enter MLOps, a merging of machine learning and DevOps principles that aims to make the deployment and maintenance of ML models more reliable and efficient.\n",
      "\n",
      "And finally, Amazon SageMaker has emerged as a leading cloud-based platform for creating, training, and deploying machine learning models. With its robust set of features and integrations, SageMaker has made it easier for developers to deploy their models in the cloud, and even on embedded systems and edge devices.\n",
      "\n",
      "But what happens when we combine these three powerful tools? What insights and possibilities can be unlocked by leveraging the strengths of Kubernetes, MLOps, and Amazon SageMaker together? Let's explore some of the possibilities in greater detail.\n",
      "\n",
      "One of the key challenges organizations face\n",
      "\n",
      "\n",
      "---------------> token = 1000\n",
      "word count = 826\n",
      "\n",
      "The world of technology is constantly evolving, and with it, new solutions and tools are emerging to help businesses stay ahead and remain competitive. Two of these powerful tools, Kubernetes and MLOps, have been gaining increasing popularity in recent years. Both are designed to streamline and automate certain aspects of software development, but with their own unique focuses. On the other hand, Amazon SageMaker is a relative newcomer in the scene, offering a cloud-based platform for deploying machine learning models. In this article, we will explore the capabilities and benefits of these three technologies and how they can work together to revolutionize the way businesses handle their operations.\n",
      "\n",
      "Kubernetes is an open-source container orchestration system that was originally designed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). Its main purpose is to automate the deployment, scaling, and management of software applications. With the rise of microservices and containerization, Kubernetes has become a go-to solution for businesses looking to modernize their IT infrastructure. Its popularity can be attributed to its ability to handle complex applications and easily scale as the needs of the business grow.\n",
      "\n",
      "One of the key benefits of Kubernetes is its self-healing capabilities. In traditional IT infrastructure, if a server goes down, the entire application may go offline. With Kubernetes, if a container fails, it automatically spins up a new one to replace it, ensuring that the application remains available. This also allows for rolling updates, where new code can be gradually rolled out to avoid any downtime.\n",
      "\n",
      "MLOps, on the other hand, stands for Machine Learning Operations. It is a paradigm that aims to streamline the development, deployment, and maintenance of machine learning models. MLOps borrows its principles from the popular software development approach, DevOps, where teams work together to continuously deliver and improve software through automated processes. In recent years, machine learning has become increasingly prevalent in various industries, and the need for a more efficient way to deploy and manage machine learning models has given rise to MLOps.\n",
      "\n",
      "One of the main benefits of MLOps is the ability to create a repeatable and standardized process for deploying machine learning models, from development to production. This means that organizations can efficiently and reliably deploy their models in real-world scenarios without manually recreating the steps each time. This enables businesses to scale their machine learning efforts and deliver more value to their customers.\n",
      "\n",
      "Finally, Amazon SageMaker is a cloud-based machine-learning platform that offers an integrated environment for data scientists and developers to build, train, and deploy machine learning models. It is designed to simplify the process of machine learning, making it accessible to a wider audience. With SageMaker, developers can quickly create their models, train them using large datasets, and deploy them to the cloud with just a few clicks.\n",
      "\n",
      "One of the key benefits of Amazon SageMaker is its ability to handle data preprocessing and feature engineering. These tasks are a crucial part of the machine learning process, and SageMaker automates them, freeing up developers' time to focus on building better models. SageMaker also offers built-in algorithms and pre-built models, making it easier for developers to get started with machine learning.\n",
      "\n",
      "Now that we have a better understanding of these three technologies, let's explore how they can work together to offer businesses a comprehensive solution for their operations. By combining Kubernetes, MLOps, and Amazon SageMaker, organizations can take their machine learning efforts to the next level.\n",
      "\n",
      "One of the most significant benefits of using Kubernetes with MLOps is the ability to easily scale machine learning models. As businesses collect more data and require more powerful models, they need a way to scale their machine learning efforts. Kubernetes is an ideal solution for this, as it can automatically spin up new containers to handle the increased workload. This means that organizations can handle an ever-growing dataset without any manual intervention. Additionally, Kubernetes can help organizations keep costs down by automatically scaling down when the workload decreases, ensuring that resources are not wasted.\n",
      "\n",
      "MLOps, with its focus on automation and continuous improvement, can bring a lot to the table when combined with Kubernetes. With MLOps, data scientists and machine learning engineers can automate the process of deploying and managing models on Kubernetes. This includes tasks like monitoring and logging, which are crucial for ensuring the reliability and performance of machine learning models. By automating these tasks, MLOps helps reduce the burden on developers and allows them to focus on building better models.\n",
      "\n",
      "The integration of Amazon SageMaker with Kubernetes and MLOps completes the trifecta of technologies that can transform the way businesses handle their operations. SageMaker offers a wide range of features that complement Kubernetes and MLOps, making it an essential part of the equation. With its powerful data preprocessing and feature engineering capabilities, SageMaker can help developers build more accurate and efficient models. Combined with Kubernetes' ability to automatically scale and MLOps' automation, this becomes a powerful tool for delivering real-world applications with machine learning capabilities.\n",
      "\n",
      "Moreover, the integration\n",
      "\n",
      "\n",
      "---------------> token = 3500\n",
      "word count = 1062\n",
      "\n",
      "Kubernetes and MLOps: The Dynamic Duo for Efficient ML Model Deployment\n",
      "\n",
      "In the ever-evolving world of technology, one of the most exciting developments in recent years has been the rise of machine learning (ML). With its ability to analyze and learn from data to make predictions and decisions, ML has become a powerful tool for businesses in various industries. However, with great power comes great responsibility, and the deployment and maintenance of ML models in production can be a daunting task. This is where Kubernetes and MLOps come in, as the dynamic duo for efficient ML model deployment. In this article, we will explore the possibilities of using these two technologies together and their potential to revolutionize the way we handle ML in production.\n",
      "\n",
      "Kubernetes, initially developed by Google, has become the de facto standard for container orchestration in the cloud. It simplifies the deployment, scaling, and management of applications by automating these processes. This allows developers to focus on writing code rather than worrying about the underlying infrastructure. On the other hand, MLOps, a term derived from the combination of \"machine learning\" and \"DevOps,\" aims to streamline the process of deploying and maintaining ML models in production. By combining the principles of DevOps with ML, MLOps automates the development and deployment of ML models, making it easier for organizations to scale their ML initiatives.\n",
      "\n",
      "So, why use Kubernetes and MLOps together? While Kubernetes handles the infrastructure layer, MLOps is responsible for the ML model layer. When used together, they form a powerful combination, and we explore below the insights and perspectives on how they can benefit businesses.\n",
      "\n",
      "Efficient Use of Resources\n",
      "One of the most significant benefits of using Kubernetes and MLOps together is the efficient use of resources. Kubernetes offers automatic scaling of resources, which means that the infrastructure can scale up or down based on demand. This is particularly useful when dealing with ML models, which can have varying resource requirements based on the dataset and complexity of the model. By using Kubernetes, organizations can ensure that they are only using the resources they need, saving them time and money.\n",
      "\n",
      "In addition, MLOps offers a streamlined process for model deployment and updates. By automating the deployment process, MLOps eliminates the need for manual intervention, which can be time-consuming and error-prone. This also leads to faster deployment times, ensuring that the latest models are available for use as soon as possible. By combining the power of Kubernetes and MLOps, organizations can deploy and manage ML models efficiently, without the need for extensive resources.\n",
      "\n",
      "Seamless Scalability\n",
      "Scalability is a crucial aspect of production ML, as models often need to handle large amounts of data and make predictions in real-time. This is where Kubernetes and MLOps shine, as they offer seamless scalability that can handle any workload. Kubernetes, with its automatic scaling capabilities, can spin up additional nodes to handle the increased workload, while MLOps handles the deployment of new models as needed.\n",
      "\n",
      "Furthermore, MLOps also offers the ability to A/B test and roll back models, ensuring that any updates or changes to the model do not have a negative impact on performance. This is especially crucial in ML, where the performance of a model can have a significant impact on the business. By using Kubernetes and MLOps together, organizations can handle varying workloads and ensure that their ML models are always performing at their best.\n",
      "\n",
      "Improved Flexibility and Portability\n",
      "In today's fast-paced business environment, agility is key, and organizations must be able to adapt quickly to changing circumstances. This is where Kubernetes and MLOps come into play, as they offer improved flexibility and portability for ML models. Kubernetes supports a wide range of platforms and services, making it easier to deploy models in different environments. This allows organizations to move their models to different cloud providers or deploy them on-premises as needed, without any significant changes to the infrastructure.\n",
      "\n",
      "Similarly, MLOps also offers flexibility and portability by automating the deployment process. This means that organizations can quickly move their models from development to production without any manual intervention, saving time and reducing the chances of errors. By using Kubernetes and MLOps, organizations can ensure that their ML models are both flexible and portable, making it easier to respond to changes in the business environment.\n",
      "\n",
      "Enhanced Collaboration and Communication\n",
      "Another insight on using Kubernetes and MLOps together is the improvement in collaboration and communication between teams. In traditional ML deployment, there can be several roadblocks in the collaboration process between data scientists, data engineers, and DevOps teams. This is because each team uses different tools and languages, hindering the communication and slowing down the deployment process.\n",
      "\n",
      "With Kubernetes and MLOps, teams can use the same platform and tooling, making collaboration and communication much smoother. This also leads to a more cohesive workflow, where teams can work together seamlessly, reducing the chances of errors and delays. By using Kubernetes and MLOps, organizations can enhance the collaboration and communication between teams, fostering a more efficient and productive work environment.\n",
      "\n",
      "Limitations and Considerations\n",
      "While Kubernetes and MLOps offer many benefits, it is essential to also consider the limitations and potential challenges of using them together. One of the main limitations is the initial learning curve for teams that are new to these technologies. Kubernetes, with its complex concepts and terminology, can be daunting for beginners, and it may take time for teams to become comfortable with it.\n",
      "\n",
      "Similarly, MLOps also has a steep learning curve, as it requires a deep understanding of ML, DevOps, and production environments. This may be challenging for organizations that do not have ML expertise in-house and may need to rely on external resources or train their teams. Furthermore, there may be compatibility issues between different versions of Kubernetes and MLOps, which may need to be resolved before deploying models.\n",
      "\n",
      "Conclusion\n",
      "The combination of Kubernetes and MLOps offers immense potential for the efficient deployment of ML models in production. By leveraging the power of Kubernetes for infrastructure management and MLOps for model deployment and updates, organizations can reap the benefits of improved resource utilization, scalability, flexibility, collaboration, and communication. However, it is essential to consider the limitations and challenges of using these technologies and ensure that teams are adequately trained to make the most out of this dynamic duo. As businesses continue to invest in ML and strive for more agile and efficient processes, the use of Kubernetes and MLOps together is bound to become a prevalent approach in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in [100, 300, 1000, 3500]:\n",
    "    response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=token  # You can adjust this parameter as needed\n",
    "    )\n",
    "    res = response[\"choices\"][0][\"text\"]\n",
    "    print()\n",
    "    print (\"---------------> token = \" + str(token))\n",
    "    print (\"word count = \" + str(len(res.split(\" \"))))\n",
    "    print (res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8i2JHQhPxRfBjXayvlh6lvGulgViD\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1705506003,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"Score: 9/10\\n\\nThe article provides a clear and concise overview of three key technologies - Kubernetes, MLOps, and Amazon SageMaker - and highlights the benefits of using them together in the deployment of machine learning models. It also provides a brief history of Kubernetes and its origins, adding depth to the discussion. Overall, the article is well-written and informative, earning a high score.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 112,\n",
      "    \"completion_tokens\": 80,\n",
      "    \"total_tokens\": 192\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "review_prompt = \"\"\"\n",
    "please offer a score for the article you generate as below:\n",
    "\n",
    "\n",
    "Kubernetes, MLOps, and Amazon SageMaker are three technologies that have transformed the way we build and deploy software in today's digital age. Each of these tools offers unique benefits to developers and businesses alike. However, when combined together, they offer a powerful solution for efficient and reliable deployment of machine learning models.\n",
    "\n",
    "Kubernetes, often referred to as \"K8s\", was developed by Google in the early 2010s as a way to manage their large-scale containerized applications.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "review_resp = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=review_prompt,\n",
    "    max_tokens=300  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "print(review_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Generate a Post in Chinese\n",
    "#----------------------------\n",
    "\n",
    "# src : https://trends.google.com.tw/trends/trendingsearches/daily?geo=TW&hl=zh-TW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024，對於許多人的想像中，可能是一個遙遠的未來。然而，在這個未來的一年，台灣卻有許多令人興奮和驚喜的事情即將發生。\n",
      "\n",
      "首先，IU的世界巡迴演唱會將再次來到台北！這位被譽為「女神」的韓國歌手，過去5年來，再也沒有踏上台灣的土地。但是，在2024年，她即將帶著她的最新專輯和粉絲們再次相會！而這也讓台灣的Uaena們非常期待和興奮。IU不僅在音樂上展現出色的天分，也常常以行動支持公益和社會公益活動，讓她更被稱為「公益女神」。而她連續釋出的多個驚喜，更\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"請產出ㄧ篇新的文章, 混合以下三篇文章\n",
    "\n",
    "\n",
    "\n",
    "1) 2024新年才剛開始，就有女神降臨台灣！昨日，IU無預警宣布今年世界巡迴演唱會的城市名單，而台北場也在其中，立刻引起台灣Uaena全體騷動！而這也是IU繼2019年來台開演唱會後，睽違5年再度登島！女神IU真的好會準備驚喜，先是年初以Uaena粉絲名義做公益、月底即將發新單曲，再來又釋出世界巡迴演唱會消息，接二連三的大禮，只能說當Uaena實在太幸福！至於IU巡迴演唱會「2024 IU H.E.R. World Tour Concert」台北場神秘嘉賓會是誰，也是值得關注的焦點！等待台灣主辦單位公布更多售票相關消息前，先趕緊下滑了解IU台北演唱會有什麼可以期待的吧！\n",
    "\n",
    "2) Lexus全新休旅車款LBX於台北車展公佈預售後，以打破級距框架的設計，並憑藉精緻且奢華的內裝質感及造車工藝，吸引大量顧客下訂，總代理和泰汽車（2207）於1月17日舉辦上市發表會，並公佈正式售價，LEXUS全新跨界休旅 ALL NEW LBX共有五個級距，自129.9萬元起跳，最高級距的車款為169萬元，於Lexus據點正式展售。\n",
    "\n",
    "NEW LBX共有五個級距全年預售目標為3000台，去年以突破3萬台的亮眼銷售成績稱霸豪華車市的Lexus，在豪華休旅級距市場亦穩坐冠軍，深受消費者喜愛的休旅車RX、NX、UX車系，銷售佔豪華休旅車的31%，稱霸豪華休旅市場。本次導入全新開發的都會跨界休旅LBX，也將提供給熱愛休旅車的消費者們另一個選擇。\n",
    "\n",
    "這款車未上市已經成為車界討論最狂熱的車款，這個生活品質與體驗，25-35歲單身或小家庭的年輕族群，50歲以上，以女生為主，家中的第二台車為主。\n",
    "\n",
    "汽車達人分析這款車是優勢，LBX命名源自Lexus Breakthrough X-over之字首，以PREMIUM CASUAL為設計概念，不受限於車身尺寸，憑藉風格且個性化的外觀設計、精緻且奢華的內裝質感與配備、更承襲了Lexus品牌一向優異的造車工藝，打造出跳脫級距框架且富有高級質感的車款，不僅賦予豪華車全新定義，更為一款引領市場潮流的全新跨界SUV車型。\n",
    "\n",
    "\n",
    "3) 2024《英雄聯盟》LCK 春季賽即將於今（17）日回歸，今天的第二個對戰組合將由 Gen.G 對上老對手 T1，本來將是一場很有可看性的頂尖對決，但對於中國網友來說，似乎卻因為 Gen.G 先前的「辱華事件」而無緣看到這次的 LCK 轉播，讓中國網友們在微博上刷「Gen.G 還我轉播」標籤而使這個話題登上微博熱搜，但現在不管要做什麼努力讓 LCK 中文台回歸看起來也有點太晚了。\n",
    "\n",
    "\n",
    "\n",
    "請提供嶄新的觀點, 從上述文章發揮\n",
    "字數至少要500字\n",
    "\"\"\"\n",
    "\n",
    "# Call the ChatGPT API using the new ChatCompletion interface\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=300  # You can adjust this parameter as needed\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "decoded_response = response['choices'][0]['text'].encode('utf-8').decode('utf-8')\n",
    "print(decoded_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------> token = 100\n",
      "word count = 84\n",
      "\n",
      "2024年的新年就帶來了許多令人期待的大事件，不僅有IU女神的演唱會和Lexus全新休旅車款的上市，還有《英雄聯盟》LCK的春季賽。然而，這些大事件背後也都有著不同的\n",
      "\n",
      "\n",
      "---------------> token = 300\n",
      "word count = 225\n",
      "\n",
      "2024年的新年對於台灣來說，可說是個開始就有無比驚喜的一年。IU作為唱跳歌手頂尖中的頂尖，除了擁有驚人的歌聲與舞蹈實力外，她的心地更是讓人佩服。在今年，IU不僅以Uaena的身份做了公益，更是將她的巨星光芒發揮到了極致，宣布將在世界巡迴演唱會中，將台北列為其中一站，再次訪問台灣，這個消息立刻引起了Uaena們的熱烈反應。\n",
      "\n",
      "對於IU來說，台灣是個充滿驚喜的地方。她在2019年來台灣開演唱會時，就被當地粉絲的熱情所感動，因此在演唱會中不時向台下\n",
      "\n",
      "\n",
      "---------------> token = 1000\n",
      "word count = 725\n",
      "\n",
      "2024年，新的一年才剛開始，卻已經有太多值得期待的事情即將發生。女神IU宣布將在今年進行世界巡迴演唱會，其中台北場也在其中，讓台灣的粉絲們激動不已。而就在這段時間，也傳出了Lexus推出全新休旅車款LBX的消息，這款車以精緻的設計和奢華的內裝吸引了許多顧客的訂購。同時，2024年的《英雄聯盟》LCK春季賽也即將開始，然而中國網友卻因為Gen.G先前的「辱華事件」而無法觀看這場備受期待的頂尖對決。在這三個事件中，我們能夠發現到一種共同的主題，那就是「追求時尚和潮流，與文化和藝術的衝突」。\n",
      "\n",
      "首先，讓我們回顧一下IU的巡迴演唱會。IU作為韓國歌壇的女神，她的歌聲、舞蹈和表演魅力吸引了全世界的粉絲。因此，她每次舉辦巡迴演唱會都會引起全球粉絲的熱烈追隨。IU除了專注於音樂，還積極參與公益活動，經常以自己的粉絲名義為公益事業做出貢獻。這顯示出她不僅是一位優秀的藝人，更是一位有社會責任感的人。而她選擇在這個時候舉辦世界巡迴演唱會，也不禁讓人想到她對於時尚和文化的追求。IU的音樂和表演不僅符合時下年輕人追求的潮流，同時也融入了韓國的文化和藝術。她的表演不僅只是一場演唱會，更是一場跨越文化界限的藝術饗宴。\n",
      "\n",
      "接下來，我們來看看Lexus的全新跨界休旅車款LBX。這款車的推出讓汽車界掀起了一股熱潮，被稱為「最具話題性的車款」。這款車不僅擁有精緻的外觀設計，更以PREMIUM CASUAL為設計概念，打破了傳統的車款級距框架。這款車結合了時尚和奢華，在融入Lexus品牌一向優異的造車工藝的同時，也為消費者提供了一種全新的生活體驗。汽車達人分析這款車的優勢，在於它能夠滿足25-35歲單身或小家庭的年輕族群和50歲以上女性消費者的不同需求。這也反\n",
      "\n",
      "\n",
      "---------------> token = 2500\n",
      "word count = 1068\n",
      "\n",
      "2024年的新年才剛開始，IU的世界巡迴演唱會即將到來，而台北場也在城市名單中。這讓Uaena全體都興奮不已，毫無疑問，女神IU在台灣有著大量的粉絲，自從2019年來台開唱以來，已經是5年難得一見的盛事。IU的熱潮也延伸到了許多年輕族群，甚至是50歲以上的女性群體，她的歌曲不僅帶給人們動聽的歌聲，更是一種生活態度的寫照。\n",
      "\n",
      "在IU開演唱會的同時，Lexus也公佈全新休旅車款LBX的上市消息。這款車以打破級距框架的設計，吸引了許多消費者的目光，總代理和泰汽車也舉辦了上市發表會。LBX共有五個級距，售價從129.9萬元起跳，最高級距的車款更是高達169萬元。這款車被稱為「休旅車中的女神」，是因為它不僅擁有獨特的外觀設計，還擁有精緻的內裝質感和高級的造車工藝，成為豪華休旅市場中的佼佼者。\n",
      "\n",
      "但是，在Lexus上市的同時，中國網友卻因為Gen.G先前的「辱華事件」而無法觀看LCK的轉播。這起事件對於中國粉絲來說，無疑是一場沉重的打擊。許多網友在微博上刷著「Gen.G還我轉播」的標籤，希望能夠重獲LCK中文台的轉播權。這也反映了中國粉絲對於電競的熱愛和對於自身國家形象的看重。\n",
      "\n",
      "但是，不管如何努力，現在希望LCK中文台能夠重回中國市場已經有點太晚了。這起事件讓人們不禁思考，電競已經走入了生活的每一個角落，無論是年輕人還是穩定的中年人，都對電競有著不可磨滅的態度。每個人都有自己熱愛的遊戲，每個人都有著自己支持的電競隊伍，這一切都是因為電競的發展已經超出了遊戲本身，成為一種生活方式。\n",
      "\n",
      "就拿IU來說，她的歌曲《Palette》曾在韓國熱搜榜上排名第一，引發大量的迴響。這首歌曲以年輕人做為主角，歌詞充滿了對於生活的迷茫和期待。電競也是如此，它讓年輕人有了一種羈絆，能夠結交志趣相投的朋友，也可以在游戲中一展身手，獲得成就感。同時，它也讓人們能夠在忙碌的生活中放鬆自己，享受一段屬於自己的時間。\n",
      "\n",
      "而Lexus的LBX則是一輛生活品質和體驗的象徵，它不僅是一款車，更是一種生活的感受。它吸引了許多年輕族群和女性消費者，展現了電競和豪華生活的奇妙結合。在這樣的時代背景下，Lexus和IU都成為了年輕人的新寵兒，帶給他們更多的選擇和生活的可能性。\n",
      "\n",
      "最後，讓我們期待IU的台北場演唱會、LBX的正式發售和LCK中文台的重回中國市場。這些都是值得期待的盛事，也象徵著電競、生活品質和娛樂產業的繁榮發展。電競已經和年輕人的生活緊密相連，而這種連結也將會更加緊密。未來，讓我們一起見證電競和現代生活的精彩融合，帶給我們更多的驚喜和感動。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in [100, 300, 1000, 2500]:\n",
    "    response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # Use the appropriate engine for ChatGPT\n",
    "    prompt=prompt,\n",
    "    max_tokens=token  # You can adjust this parameter as needed\n",
    "    )\n",
    "    #res = response[\"choices\"][0][\"text\"]\n",
    "    decoded_response = response['choices'][0]['text'].encode('utf-8').decode('utf-8')\n",
    "    print()\n",
    "    print (\"---------------> token = \" + str(token))\n",
    "    print (\"word count = \" + str(len(decoded_response)))\n",
    "    print (decoded_response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
